{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../datasets/AAAI_dataset/gossip_train.csv\")\n",
    "df_test = pd.read_csv(\"../datasets/AAAI_dataset/gossip_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callable image_transform with Compose\n",
    "image_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../datasets/AAAI_dataset/Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # when loading file paths\n",
    "import pandas as pd  # for lookup in annotation file\n",
    "import spacy  # for tokenizer\n",
    "import torch\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load img\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext.vocab as vocab\n",
    "from torchtext.data import Field\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "spacy_eng = spacy.load(\"en\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, embed_size):\n",
    "        self.itos = {0: \"<PAD>\"}\n",
    "        self.stoi = {\"<PAD>\": 0}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.embed_size = embed_size\n",
    "        self.pre_trained_embed = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_w2v(self, sentence_list):\n",
    "        sentences = [self.tokenizer_eng(sentence) for sentence in sentence_list]\n",
    "        # train model\n",
    "        model = Word2Vec(sentences, min_count=self.freq_threshold, size=self.embed_size)  \n",
    "\n",
    "#         print(model.wv.vocab)  \n",
    "\n",
    "        model.save('embeddings.txt')\n",
    "\n",
    "        TEXT = Field()\n",
    "        TEXT.build_vocab(sentences, min_freq=self.freq_threshold)\n",
    "\n",
    "        w2v_vec = []\n",
    "        for token, idx in self.stoi.items():\n",
    "            if token in model.wv.vocab.keys():\n",
    "                w2v_vec.append(torch.FloatTensor(model.wv[token]))\n",
    "            else:\n",
    "                w2v_vec.append(torch.zeros(self.embed_size))\n",
    "\n",
    "        TEXT.vocab.set_vectors(self.stoi, w2v_vec, self.embed_size)\n",
    "\n",
    "        self.pre_trained_embed = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "        \n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 1\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else random.choice(list(transformed_dataset_train.vocab.stoi.values()))\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    \"\"\"Fake News Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, image_transform, vocab=None, freq_threshold=5, embed_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with text and img name.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Get img, caption columns\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.txt = self.df[\"content\"]\n",
    "        self.label = self.df['label']\n",
    "        \n",
    "        if self.vocab == None:\n",
    "        # Initialize vocabulary and build vocab\n",
    "            self.vocab = Vocabulary(freq_threshold,  embed_size)\n",
    "            self.vocab.build_vocabulary(self.txt.tolist())\n",
    "\n",
    "            self.vocab.build_w2v(self.txt.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def pre_processing_text(self, sent):\n",
    "        pass\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_name = self.root_dir + self.imgs[idx]\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        text = self.txt[idx]\n",
    "        \n",
    "        # numericalized_text = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        # numericalized_text += self.vocab.numericalize(text)\n",
    "        numericalized_text = self.vocab.numericalize(text)\n",
    "        # numericalized_text.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "\n",
    "        label = self.label[idx]\n",
    "        label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        sample = {'image': image, 'text': torch.tensor(numericalized_text), 'label': label}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \n",
    "#         print(batch)\n",
    "        imgs = [item['image'].unsqueeze(0) for item in batch]\n",
    "        \n",
    "        labels = [item['label'].unsqueeze(0) for item in batch]\n",
    "        text = [item['text'] for item in batch]\n",
    "    \n",
    "#         batch_imgs, batch_text, batch_labels = batch['image'], batch['text'], batch['label']\n",
    "\n",
    "#         imgs = [item.unsqueeze(0) for item in batch_imgs]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "#         labels = [item.unsqueeze(0) for item in batch_labels]\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "\n",
    "#         text = [item for item in batch_text]\n",
    "        text = pad_sequence(text, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "\n",
    "        sample = {'image': imgs, 'text': text, 'label': labels}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    df,\n",
    "    root_dir,\n",
    "    image_transform,\n",
    "    vocab=None,\n",
    "    batch_size=8,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FakeNewsDataset(df, root_dir, image_transform, vocab=None, freq_threshold=5, embed_size=32)\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "#         num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "#         pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muditd/.conda/envs/fn/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run function `preprocessing_for_bert` on the dataset\n",
    "train_dataloader, transformed_dataset_train = get_loader(df=df_train, root_dir=root_dir+\"gossip_train/\", image_transform=image_transform, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muditd/.conda/envs/fn/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataloader, transformed_dataset_test = get_loader(df=df_test, root_dir=root_dir+\"gossip_test/\", image_transform=image_transform, vocab=transformed_dataset_train.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   19,    20,    21,    24,    15,    16,  1594,     8,     3,    36,\n",
      "           23,    13,    28,  5062,     7,   777,    14, 15283,  1059,    55,\n",
      "           20,    25,  1318,    10,     4,    25,    15,    16, 13167,    58,\n",
      "           26,     9,  1231,     3,     6,    75,    13,   439,     1,    30,\n",
      "           29,     1,    73, 16744, 16410,   646,     2,    17,    21,     5,\n",
      "          280,    27,  2412,  2414,    35,    19,    20,    21,  2156,    37,\n",
      "           10,     4,     1,  2125,     2,  1866,     2,   223, 11305,    17,\n",
      "            7,  9569,  4448,     2,    99,    75,    13, 26614,     1,  1247,\n",
      "           14,    34,     9, 10809,    11,    18,     1,    36,    23,     3,\n",
      "            1,   119,  3455,    32,    28,   511,     2,    26,     1,    83,\n",
      "          627,    33,     8,     5,     9,    31,  3034,     6,    10,     4,\n",
      "         1889,   466,    27,   136,     4,    11,    24,    15,    16,    48,\n",
      "            7,    11,    18,     1,    36,    23,     3,     1,    21,  3455,\n",
      "           32,    27,     1,    19,    20,   162,    29,     1,    73, 16744,\n",
      "        16410,   646,     5,   280,    14,   627,    33,  4799,     5,     1,\n",
      "           38,     3,     9,     6,    22,    13,   104,    18,     1,    30,\n",
      "            4,     8,    24,     1,    31,     6,   369,     7,   599,     3,\n",
      "            8,     5,     1,    38,     4,     1,    31,     6,   369,     7,\n",
      "          599,     3,     8,     5,     1,    38,    25,   840,     1,  5119,\n",
      "           25, 12183,     2,    26,   665,   561, 14848,     1, 17182,    11,\n",
      "          235,    10,     4,    12,    15,    16,    90,    84,    23,  1804,\n",
      "           17,     1,  1340,     3,     7,    98,     5,     1,    38,     3,\n",
      "            9,     6,   100,    63,  2504,     2,    12,  2502,    19,   642,\n",
      "            2,   168,   225,  2746,    24,     4,    12, 10056,    35,     6,\n",
      "        28805,   266,    51,   615,    33,   488,    10,     4,    12,   814,\n",
      "            7,  1016,     2,   669,  2333,     2,    11,    10,   742,  1431,\n",
      "         1437, 18056,    10,    12,     4,     1,    83,   256,    48,     9,\n",
      "         3361,    14,  7836,     2,  3637,  1177,   875,  3749,    68,   223,\n",
      "         2098,    22,    13,   788,  2486,    27,     1,   502,    10,     4,\n",
      "            1,    46,  1944,     7,   269,   187,  1531,     9,  5436,   286,\n",
      "        11028,   532,    17,     7, 10574,   552,    22,    13,   725,     1,\n",
      "          847,     5,     9,    97,   413, 10575,    26,  2736,   227,    14,\n",
      "           34,    10,     4, 14424,   683,    79,    15,    16,   223, 14741,\n",
      "           17,     1, 10577,  1924,  2088,  4448,    52,  1076,    18,   285,\n",
      "            9,  2928,    22,    13,   104,    18,     1,    30,    29,     1,\n",
      "           19,    20,    21,   162,     5,   280,     4,    15,    28,   631,\n",
      "        14399,    69,     9,   568,    61,     1,  2321,     3,     9,    98,\n",
      "           22,     7,    11,    18,     1,    23,     3,     1,  3455,    32,\n",
      "            2,    39,   839,     1,   502,    24,    12,    20, 17677,  2429,\n",
      "           25,    20, 17677,  2604,    12,    10,     4,  1742,     3,     9,\n",
      "           70,     2,    13,   173,     5,   194,    24,     4,    12,     1,\n",
      "          111,  1483,  3579,   211,    26,    20,    25,    17,  6137,    10,\n",
      "           46,   735,  6429,    18,    67,     7,   165,     3,    81,    80,\n",
      "           10,    12,     4,  5181,    24,     1, 13245,   187,  7113,     9,\n",
      "        12887,  5002,    17,     1, 10573,  3150,     9, 21742,  1545,     2,\n",
      "         1095,     7,   246,     3,  1177,   875,  3178,    68,    27,     1,\n",
      "         3929,     4,    13,    35,   279,   552,    24,    13,  1531,     9,\n",
      "          286, 11028,   532,    17,     7, 10574,   552,    22,    13, 12003,\n",
      "           69,     9,   568,     4,  1005,    24,     1,    46,  1944,     7,\n",
      "          269,   187,    28,  1154,    22,    13,   457,     1,    70,     3,\n",
      "            1,   198,     4,  3000,    24,     1,   951,  1550,   187,    28,\n",
      "            7,  4573,    17,    31,    22,    13, 17773,     1,  1054,     3,\n",
      "            1,   138,     4,     9,    70,   332,     7,   537,     3,   112,\n",
      "           60,   108,   869,     2, 10386,    25,  1313,  1314,    10,     4,\n",
      "            1,    11,   162,     2,  1028,    29, 22067,    73, 16744, 16410,\n",
      "          646,     2,    28,  7339,   749,    49,  2412,     5,   554,    10,\n",
      "            4,     1,  2789,  2496,    14,    73,   781,     5,   194,    40,\n",
      "           13,   200,  1132,     1,    19,    20,  7820,    27,     1,    23,\n",
      "            3,     1,    32,    14,     7,  2794,   141,   508,    39,     1,\n",
      "         2631,   469, 10388,    40,  4727,    29,     9,    21,  2413,    37,\n",
      "            2,     5,  1130,   119,    36,    10,     4,  1625,  6256,    24,\n",
      "            1,   502,   424,   643,    22,    13,  2571,     9,    98,    69,\n",
      "            5,    30,     2, 10560,     9,  2775,   292,     1,  1717,     4,\n",
      "         6912,    24,     1,  2114,  5904,  2922,    34,     9,  5002,    26,\n",
      "            7,  9569,  4213,     4,   274,    68,   230,    48,    27,   871,\n",
      "           79,     1,  2125, 20390,   840,     1,    30,    17,    31,  3637,\n",
      "         3749,    68,     4,  1318,    79,    13,  2131,     9,   684,    25,\n",
      "          457,     1,  1318,   785,    22,     7,    11,    18,     1,    23,\n",
      "            3,     1,    32,     4,  1197,     9,   144,     2,  1908,  3592,\n",
      "           14, 23292,  1313,  1314,     2,   252,   344,     2,  7717,     2,\n",
      "         4938,     2,  6506,   503,     2,   195,    40,    25,  8438,  8439,\n",
      "           82,   781,    33,  6362,    49,  1092,     1,    37,    35,  7470,\n",
      "         8355,    18,    33,   624,   625,  4285,    10,     4,    84,  8335,\n",
      "          152,   294,   609,    18,     1,    12,   115,    20,    21,  1700,\n",
      "         6230,    12,     2,    52,    41,    93,   144,    34,    49,     1,\n",
      "           21,   707,  2257,    17,  4642,    26,     1,   540,   352,   360,\n",
      "           18,   719,  9604,  2871,    25,    23,     3,     1,    32,    10,\n",
      "            4,  1956,    24,    13, 10575,     9,    97,  4159,   258,    26,\n",
      "         2736,   227,    14,    34,     4,    83,    14,  1714,   378,    14,\n",
      "           34,    24,     9,    70,   332,     7,   537,     3,   112,    60,\n",
      "          108,   869,     2, 10386,    25,  1313,  1314,     4,     1,  2631,\n",
      "        10388,   104,   172,    29,  2412,    35,    21,    37,   718,    81,\n",
      "          686,     2,  3201,    36,   283,     2,    60,   212,     2,    25,\n",
      "        16412, 23060,    10,     4,   469,  9318, 21389, 13875,     2,    36,\n",
      "            2,    90,  6320,    37,    14, 15542,    49,  2675,   191,     7,\n",
      "           12,  4289,  7284,    12,    17,     1,  2695,     3,     1,    21,\n",
      "         2413,     2,  2251,    39,  2412,    35,   144,  1187,    10,     4,\n",
      "         2403,   136,  1560,   381, 18175,  1127,  5764,    25, 16383,    27,\n",
      "            1,  4849,     2,    39,  1875,     1,  2129,     3,     1,  7740,\n",
      "        16427,   263,     1,  4426,    35,  5615,    10,     4,     1, 26048,\n",
      "         3909,  3846,    35, 18513,    32,   207,     1, 10626, 21929,     5,\n",
      "            1,   766, 12709,     2,    52,  1943,  4249,    38,    17,  1243,\n",
      "           10,     4,   762,   222,    24,    15,    90,   392,    76,    13,\n",
      "           28,    12,  6429,    12,    18,    67,   165,     3,     1,  1201])\n"
     ]
    }
   ],
   "source": [
    "# transformed_dataset_train.__getitem__(2)['text']\n",
    "print(transformed_dataset_train.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[ 2.1462,  2.1633,  2.1633,  ...,  1.7352,  1.7523,  1.7523],\n",
      "          [ 2.1462,  2.1633,  2.1633,  ...,  1.7523,  1.7523,  1.7352],\n",
      "          [ 2.1462,  2.1804,  2.1633,  ...,  1.7352,  1.7352,  1.7180],\n",
      "          ...,\n",
      "          [ 1.6495,  1.6838,  1.7009,  ..., -1.2103, -1.2103, -1.2103],\n",
      "          [ 1.6838,  1.7180,  1.6838,  ..., -0.5253, -0.5596, -0.5596],\n",
      "          [ 1.6667,  1.7009,  1.6838,  ..., -0.1314, -0.1314, -0.1143]],\n",
      "\n",
      "         [[ 2.0784,  2.0959,  2.0959,  ...,  1.4482,  1.4132,  1.4132],\n",
      "          [ 2.0784,  2.0959,  2.0959,  ...,  1.4657,  1.4132,  1.3957],\n",
      "          [ 2.0784,  2.1134,  2.0959,  ...,  1.4657,  1.4482,  1.4307],\n",
      "          ...,\n",
      "          [ 1.7283,  1.7458,  1.7808,  ..., -1.1779, -1.1779, -1.1779],\n",
      "          [ 1.7283,  1.7458,  1.7633,  ..., -0.5651, -0.5826, -0.6001],\n",
      "          [ 1.7108,  1.7283,  1.7283,  ..., -0.1975, -0.1625, -0.1450]],\n",
      "\n",
      "         [[ 1.6814,  1.6988,  1.6988,  ...,  0.5659,  0.5659,  0.5834],\n",
      "          [ 1.6814,  1.6988,  1.6988,  ...,  0.5834,  0.5834,  0.5659],\n",
      "          [ 1.6814,  1.7163,  1.6988,  ...,  0.5834,  0.6008,  0.5834],\n",
      "          ...,\n",
      "          [ 1.8731,  1.9080,  1.9080,  ..., -1.1421, -1.1421, -1.1073],\n",
      "          [ 1.8731,  1.9080,  1.8557,  ..., -0.6193, -0.6367, -0.6193],\n",
      "          [ 1.8208,  1.8557,  1.8383,  ..., -0.3055, -0.2184, -0.2532]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2899,  1.2899,  1.3070,  ...,  1.3755,  1.8893,  1.9749],\n",
      "          [ 1.2899,  1.2899,  1.2899,  ...,  1.3584,  1.8722,  1.9578],\n",
      "          [ 1.3070,  1.2728,  1.2899,  ...,  1.3584,  1.8379,  1.9578],\n",
      "          ...,\n",
      "          [ 1.6153,  1.5982,  0.9988,  ..., -0.2856, -0.6281, -0.9534],\n",
      "          [ 1.5982,  1.6838,  1.0502,  ...,  1.1700,  1.1529,  1.0844],\n",
      "          [ 1.5982,  1.7009,  1.0159,  ...,  0.9817,  1.0159,  1.0502]],\n",
      "\n",
      "         [[ 1.2031,  1.2031,  1.2206,  ...,  1.3782,  1.9034,  1.9734],\n",
      "          [ 1.2031,  1.2031,  1.2031,  ...,  1.3606,  1.8859,  1.9559],\n",
      "          [ 1.2206,  1.1856,  1.2031,  ...,  1.3606,  1.8508,  1.9559],\n",
      "          ...,\n",
      "          [ 0.5378,  0.7304, -0.2850,  ..., -0.1625, -0.5301, -0.8452],\n",
      "          [ 0.5553,  0.8004, -0.2500,  ...,  1.2731,  1.2381,  1.1681],\n",
      "          [ 0.5728,  0.8004, -0.2850,  ...,  1.0630,  1.0805,  1.1155]],\n",
      "\n",
      "         [[ 1.2631,  1.2631,  1.2805,  ...,  1.4722,  2.0300,  2.1346],\n",
      "          [ 1.2631,  1.2631,  1.2631,  ...,  1.4374,  2.0125,  2.1346],\n",
      "          [ 1.2805,  1.2457,  1.2631,  ...,  1.4548,  1.9777,  2.1171],\n",
      "          ...,\n",
      "          [ 0.8622,  0.8971, -0.2184,  ...,  0.0431, -0.3230, -0.6367],\n",
      "          [ 0.7402,  0.9494, -0.2184,  ...,  1.3851,  1.3851,  1.3328],\n",
      "          [ 0.7054,  0.9494, -0.2358,  ...,  1.1585,  1.2282,  1.2805]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0948,  2.2147,  2.1975,  ...,  2.1290,  1.9578,  2.0605],\n",
      "          [ 1.4440,  1.7009,  2.1462,  ...,  2.1633,  2.0605,  2.1119],\n",
      "          [ 1.7865,  1.5982,  1.9064,  ...,  2.1804,  2.1462,  2.0948],\n",
      "          ...,\n",
      "          [ 1.1700,  0.5878,  1.3070,  ..., -1.1932, -1.5528, -1.3302],\n",
      "          [ 0.9132,  1.0673,  1.2557,  ..., -1.1075, -1.3815,  0.2111],\n",
      "          [ 0.7591,  0.7248,  0.8104,  ..., -0.7822, -0.8335,  0.9817]],\n",
      "\n",
      "         [[ 2.2885,  2.3761,  2.3936,  ...,  2.2710,  2.0959,  2.2360],\n",
      "          [ 1.7633,  1.9909,  2.3585,  ...,  2.2535,  2.1660,  2.2360],\n",
      "          [ 2.1134,  1.9384,  2.1660,  ...,  2.3410,  2.2710,  2.2185],\n",
      "          ...,\n",
      "          [ 1.5357,  0.9230,  1.5182,  ..., -1.0903, -1.3880, -1.0378],\n",
      "          [ 1.2906,  1.4307,  1.5882,  ..., -0.8978, -1.1779,  0.5028],\n",
      "          [ 1.1681,  1.1331,  1.2556,  ..., -0.5126, -0.6176,  1.1681]],\n",
      "\n",
      "         [[ 2.5354,  2.5877,  2.5703,  ...,  2.5529,  2.4483,  2.5180],\n",
      "          [ 2.3786,  2.4308,  2.5703,  ...,  2.5529,  2.4657,  2.5354],\n",
      "          [ 2.5529,  2.4657,  2.5354,  ...,  2.5703,  2.5354,  2.5354],\n",
      "          ...,\n",
      "          [ 2.1694,  1.5942,  1.9428,  ..., -0.6541, -1.1421, -0.7238],\n",
      "          [ 2.0125,  2.0648,  2.1520,  ..., -0.4798, -0.7936,  0.9145],\n",
      "          [ 1.8905,  1.8731,  1.9603,  ..., -0.0092, -0.2358,  1.6988]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1008, -2.0837],\n",
      "          ...,\n",
      "          [-1.3815, -1.3815, -1.4158,  ..., -1.6555, -1.5699, -1.5185],\n",
      "          [-1.3815, -1.3644, -1.3987,  ..., -1.6384, -1.5699, -1.5185],\n",
      "          [-1.3644, -1.3644, -1.3815,  ..., -1.6213, -1.5699, -1.5528]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0357, -2.0182],\n",
      "          ...,\n",
      "          [-1.2654, -1.2654, -1.3004,  ..., -1.6331, -1.5630, -1.5280],\n",
      "          [-1.2654, -1.2479, -1.2829,  ..., -1.6155, -1.5630, -1.5455],\n",
      "          [-1.2479, -1.2479, -1.2654,  ..., -1.5980, -1.5630, -1.5805]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.7696, -1.7522, -1.7870],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.7696, -1.7522, -1.7347],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.7522, -1.7173, -1.6650],\n",
      "          ...,\n",
      "          [ 1.4374,  1.4374,  1.4025,  ..., -0.7587, -0.6715, -0.5844],\n",
      "          [ 1.4374,  1.4548,  1.4200,  ..., -0.7587, -0.6715, -0.6018],\n",
      "          [ 1.4548,  1.4548,  1.4374,  ..., -0.7238, -0.6715, -0.6193]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2214,  1.2214,  1.2557,  ...,  1.3413,  1.3927,  1.4954],\n",
      "          [ 1.2385,  1.2385,  1.2385,  ...,  1.5982,  1.7523,  1.7352],\n",
      "          [ 1.2385,  1.2557,  1.2385,  ...,  1.6153,  1.7009,  1.6667],\n",
      "          ...,\n",
      "          [ 1.2385,  1.2214,  1.2385,  ...,  1.6153,  1.6153,  1.6324],\n",
      "          [ 1.2557,  1.2385,  1.2557,  ...,  1.6153,  1.6153,  1.6153],\n",
      "          [ 1.2043,  1.2557,  1.2385,  ...,  1.6153,  1.6153,  1.6324]],\n",
      "\n",
      "         [[-1.6681, -1.6331, -1.6856,  ..., -0.5651, -1.3179, -1.4055],\n",
      "          [-1.6681, -1.6506, -1.7031,  ..., -0.8277, -1.3354, -1.3529],\n",
      "          [-1.6681, -1.6681, -1.7031,  ..., -0.8277, -1.3354, -1.3529],\n",
      "          ...,\n",
      "          [-1.7031, -1.7381, -1.7906,  ..., -1.5280, -1.5105, -1.4930],\n",
      "          [-1.7031, -1.7556, -1.7731,  ..., -1.5280, -1.5105, -1.5105],\n",
      "          [-1.7556, -1.7381, -1.7906,  ..., -1.5280, -1.5105, -1.4930]],\n",
      "\n",
      "         [[-1.7173, -1.6824, -1.6999,  ..., -0.3055, -1.0898, -1.1944],\n",
      "          [-1.6999, -1.6824, -1.7347,  ..., -0.8284, -1.4210, -1.4907],\n",
      "          [-1.7173, -1.7173, -1.7347,  ..., -0.8458, -1.4036, -1.4036],\n",
      "          ...,\n",
      "          [-1.7173, -1.7173, -1.7173,  ..., -1.4907, -1.4907, -1.4733],\n",
      "          [-1.6824, -1.6999, -1.6999,  ..., -1.5081, -1.4907, -1.4907],\n",
      "          [-1.7347, -1.6824, -1.7173,  ..., -1.4907, -1.4907, -1.4733]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9474,  0.8618,  0.8104,  ...,  1.0844,  1.1187,  1.1187],\n",
      "          [ 0.7762,  0.7077,  0.7591,  ...,  1.1015,  1.1187,  1.1187],\n",
      "          [ 0.6392,  0.6563,  0.7591,  ...,  1.1015,  1.1187,  1.1187],\n",
      "          ...,\n",
      "          [ 1.5297,  1.5297,  1.5297,  ...,  1.2043,  1.2043,  1.1872],\n",
      "          [ 1.5297,  1.5297,  1.5297,  ...,  1.2043,  1.2043,  1.1872],\n",
      "          [ 1.5297,  1.5297,  1.5297,  ...,  1.2214,  1.2214,  1.2043]],\n",
      "\n",
      "         [[ 1.1681,  1.1506,  1.1681,  ...,  1.3606,  1.3957,  1.3957],\n",
      "          [ 1.0630,  1.0280,  1.1155,  ...,  1.3782,  1.3957,  1.3957],\n",
      "          [ 0.9405,  0.9580,  1.0805,  ...,  1.3782,  1.3957,  1.3957],\n",
      "          ...,\n",
      "          [ 1.7108,  1.7108,  1.7108,  ...,  1.4832,  1.4832,  1.4657],\n",
      "          [ 1.7108,  1.7108,  1.7108,  ...,  1.4832,  1.4832,  1.4657],\n",
      "          [ 1.7108,  1.7108,  1.7108,  ...,  1.5007,  1.5007,  1.4832]],\n",
      "\n",
      "         [[ 1.4897,  1.4548,  1.4374,  ...,  1.6814,  1.7163,  1.7163],\n",
      "          [ 1.3677,  1.3328,  1.3851,  ...,  1.6988,  1.7163,  1.7163],\n",
      "          [ 1.2457,  1.2631,  1.3677,  ...,  1.6988,  1.7163,  1.7163],\n",
      "          ...,\n",
      "          [ 1.9951,  1.9951,  1.9951,  ...,  1.8034,  1.8208,  1.8557],\n",
      "          [ 1.9951,  1.9951,  1.9951,  ...,  1.8034,  1.8208,  1.8208],\n",
      "          [ 1.9951,  1.9951,  1.9951,  ...,  1.8208,  1.8208,  1.8034]]]]), 'text': tensor([[ 2958,    53,    45,  ..., 13044,  3851,   155],\n",
      "        [   81,  1045,     3,  ...,     0,     0,     0],\n",
      "        [  651,   290,   149,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [12535, 26499,   152,  ...,     0,     0,     0],\n",
      "        [  941,  1661,  3729,  ...,     0,     0,     0],\n",
      "        [   12,  3403,    12,  ...,     0,     0,     0]]), 'label': tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])}\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(train_dataloader):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3af0b15ceeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(batch['label'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7110371876df>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumericalized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "data = iter(train_dataloader)\n",
    "batch = iter(test_dataloader).next()\n",
    "# print(batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(transformed_dataset_train.vocab.stoi.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding.from_pretrained(transformed_dataset_train.vocab.pre_trained_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding(batch['text']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnd_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict_model = {\n",
    "    'pre_trained_embed': transformed_dataset_train.vocab.pre_trained_embed,\n",
    "    'latent_dim': 32,\n",
    "    'combined_fc_out': 64,\n",
    "    'dec_fc_img_1': 1024,\n",
    "    'enc_img_dim': 4096,\n",
    "    'vocab_size': len(transformed_dataset_train.vocab.stoi),\n",
    "    'embedding_size': 32,\n",
    "    'max_len': 20,\n",
    "    'text_enc_dim': 32,\n",
    "    'latent_size': 32,\n",
    "    'hidden_size': 32,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': True,\n",
    "    'img_fc1_out': 1024,\n",
    "    'img_fc2_out': 32,\n",
    "    'fnd_fc1': 64,\n",
    "    'fnd_fc2': 32\n",
    "}\n",
    "\n",
    "parameter_dict_opt={'l_r': 3e-5,\n",
    "                    'eps': 1e-8\n",
    "                    }\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31298"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_dict_model['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torchvision import models, transforms\n",
    "\n",
    "\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self, pre_trained_embed, vocab_size, embedding_size, text_enc_dim, num_layers, hidden_size, bidirectional):\n",
    "#         super(TextEncoder, self).__init__()\n",
    "\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.num_layers = num_layers\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding.from_pretrained(pre_trained_embed)\n",
    "\n",
    "#         self.hidden_factor = (2 if bidirectional else 1) * num_layers\n",
    "\n",
    "#         self.text_encoder = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, bidirectional=True,\n",
    "#                                batch_first=True)\n",
    "\n",
    "#         self.text_enc_fc = torch.nn.Linear(self.hidden_size*self.hidden_factor, text_enc_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.embedding(x)\n",
    "        \n",
    "# #         print(\"emb \", x.shape)\n",
    "\n",
    "#         _, (hidden, _not) = self.text_encoder(x)\n",
    "        \n",
    "# #         print(\"encoding hidden\", hidden.shape)\n",
    "\n",
    "#         if self.bidirectional or self.num_layers > 1:\n",
    "#             # flatten hidden state\n",
    "#             hidden = hidden.view(x.shape[0], self.hidden_size*self.hidden_factor)\n",
    "#         else:\n",
    "#             hidden = hidden.squeeze()\n",
    "        \n",
    "# #         print(\"encoding hidden\", hidden.shape)\n",
    "        \n",
    "#         x = self.text_enc_fc(hidden)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class VisualEncoder(nn.Module):\n",
    "#     def __init__(self, enc_img_dim, img_fc1_out, img_fc2_out):\n",
    "#         super(VisualEncoder, self).__init__()\n",
    "        \n",
    "#         vgg = models.vgg19(pretrained=True)\n",
    "#         vgg.classifier = nn.Sequential(*list(vgg.classifier.children())[:1])\n",
    "        \n",
    "#         self.vis_encoder = vgg\n",
    "\n",
    "#         self.vis_enc_fc1 = torch.nn.Linear(enc_img_dim, img_fc1_out)\n",
    "\n",
    "#         self.vis_enc_fc2 = torch.nn.Linear(img_fc1_out, img_fc2_out)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x_cnn = self.vis_encoder(x)\n",
    "\n",
    "#         x = self.vis_enc_fc1(x_cnn)\n",
    "\n",
    "#         x = self.vis_enc_fc2(x)\n",
    "\n",
    "#         return x, x_cnn\n",
    "\n",
    "# class VisualDecoder(nn.Module):\n",
    "#     def __init__(self, latent_dim, dec_fc_img_1, decoded_img):\n",
    "#         super(VisualDecoder, self).__init__()\n",
    "\n",
    "#         self.vis_dec_fc1 = nn.Linear(latent_dim, dec_fc_img_1)\n",
    "\n",
    "#         self.vis_dec_fc2 = nn.Linear(dec_fc_img_1, decoded_img)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.vis_dec_fc1(x)\n",
    "\n",
    "#         x = self.vis_dec_fc2(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class TextDecoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_size, max_len, latent_size, hidden_size, num_layers, bidirectional):\n",
    "#         super(TextDecoder, self).__init__()\n",
    "\n",
    "#         self.max_len = max_len\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.num_layers = num_layers\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.vocab_size = vocab_size\n",
    "\n",
    "#         self.hidden_factor = (2 if bidirectional else 1) * num_layers\n",
    "\n",
    "#         self.text_decoder = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional,\n",
    "#                                batch_first=True)\n",
    "\n",
    "#         self.latent2hidden = nn.Linear(latent_size, hidden_size )  ## dec text fc \n",
    "\n",
    "#         self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "    \n",
    "#     def forward(self, x, max_length):\n",
    "\n",
    "#         hidden = self.latent2hidden(x)\n",
    "# #         print(\"hidden shgape \",hidden.shape)\n",
    "# #         print(\"max len \", max_length)\n",
    "\n",
    "# #         if self.bidirectional or self.num_layers > 1:\n",
    "# #             # unflatten hidden state\n",
    "# #             hidden = hidden.view(self.hidden_factor, x.shape[0], self.hidden_size)\n",
    "# #         else:\n",
    "# #             hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "# #         print(\"hidden shgape \",hidden.shape)\n",
    "        \n",
    "# #         hidden = hidden.unsqueeze(1)\n",
    "        \n",
    "# #         print(\"hidden shgape unsqueezed\",hidden.shape)\n",
    "        \n",
    "#         repeat_hidden = hidden.unsqueeze(1).repeat(1, max_length, 1)  ## repeat the hidden input to the max_len\n",
    "\n",
    "#         # decoder forward pass\n",
    "#         outputs, _ = self.text_decoder(repeat_hidden)\n",
    "        \n",
    "#         outputs = outputs.contiguous()\n",
    "#         print(\"outputs shape after lstm \", outputs.shape)\n",
    "\n",
    "#         b,s,_ = outputs.size()\n",
    "\n",
    "#         # project outputs to vocab\n",
    "# #         logp = nn.functional.log_softmax(self.outputs2vocab(outputs.view(-1, outputs.size(2))), dim=1)\n",
    "#         logp = nn.functional.log_softmax(self.outputs2vocab(outputs), dim=-1)\n",
    "#         print(\"logp shape before \", logp.shape)\n",
    "#         logp = logp.view(b, s, self.vocab_size)\n",
    "#         print(\"logp shape after \", logp.shape)\n",
    "        \n",
    "#         return logp\n",
    "\n",
    "\n",
    "# class MVAE(nn.Module):\n",
    "\n",
    "#     def __init__(self, params_dict):\n",
    "#         super(MVAE, self).__init__()\n",
    "\n",
    "#         self.text_encoder = TextEncoder(params_dict['pre_trained_embed'], params_dict['vocab_size'], params_dict['embedding_size'], params_dict['text_enc_dim'], params_dict['num_layers'], params_dict['hidden_size'], params_dict['bidirectional'])\n",
    "\n",
    "#         self.visual_encoder = VisualEncoder(params_dict['enc_img_dim'], params_dict['img_fc1_out'], params_dict['img_fc2_out'])\n",
    "\n",
    "#         self.text_decoder = TextDecoder(params_dict['vocab_size'], params_dict['embedding_size'], params_dict['max_len'], params_dict['latent_dim'], params_dict['hidden_size'], params_dict['num_layers'], params_dict['bidirectional'])\n",
    "\n",
    "#         self.visual_decoder = VisualDecoder(params_dict['latent_dim'], params_dict['dec_fc_img_1'], params_dict['enc_img_dim'])\n",
    "\n",
    "#         self.combined_fc = torch.nn.Linear((params_dict['text_enc_dim'] + params_dict['img_fc2_out']), params_dict['combined_fc_out'])\n",
    "\n",
    "#         self.fc_mu = nn.Linear(params_dict['combined_fc_out'], params_dict['latent_dim'])\n",
    "#         self.fc_var = nn.Linear(params_dict['combined_fc_out'], params_dict['latent_dim'])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#         self.fnd_module = nn.Sequential(\n",
    "#                             nn.Linear(params_dict['latent_dim'], params_dict['fnd_fc1']),\n",
    "#                             nn.Tanh(),\n",
    "#                             nn.Linear(params_dict['fnd_fc1'], params_dict['fnd_fc2']),\n",
    "#                             nn.Tanh(),\n",
    "#                             nn.Linear(params_dict['fnd_fc2'], 1),\n",
    "#                             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "        \n",
    "\n",
    "#     def encode(self, text_ip, img_ip):\n",
    "#         encoded_text = self.text_encoder(text_ip)\n",
    "\n",
    "#         encoded_img, cnn_enc_img = self.visual_encoder(img_ip)\n",
    "\n",
    "#         combined = torch.cat(\n",
    "#             [encoded_text, encoded_img], dim=1\n",
    "#         )\n",
    "\n",
    "#         result = self.combined_fc(combined)\n",
    "\n",
    "#         # Split the result into mu and var components\n",
    "#         # of the latent Gaussian distribution\n",
    "#         mu = self.fc_mu(result)\n",
    "#         log_var = self.fc_var(result)\n",
    "\n",
    "#         return cnn_enc_img, mu, log_var\n",
    "\n",
    "\n",
    "#     def decode(self, z, max_len):\n",
    "#         recon_text = self.text_decoder(z, max_len)\n",
    "\n",
    "#         recon_img = self.visual_decoder(z)\n",
    "\n",
    "#         return [recon_text, recon_img]\n",
    "    \n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         \"\"\"\n",
    "#         Reparameterization trick to sample from N(mu, var) from\n",
    "#         N(0,1).\n",
    "#         :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "#         :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "#         :return: (Tensor) [B x D]\n",
    "#         \"\"\"\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return eps * std + mu\n",
    "    \n",
    "    \n",
    "#     def forward(self, text_ip, img_ip):\n",
    "\n",
    "#         ## encoder network \n",
    "#         cnn_enc_img, mu, log_var = self.encode(text_ip, img_ip)\n",
    "\n",
    "#         z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "# #         print(\"text ip shape\",text_ip.shape)\n",
    "\n",
    "#         recon_text, recon_img = self.decode(z, text_ip.shape[1])\n",
    "\n",
    "#         fnd_out = self.fnd_module(z)\n",
    "\n",
    "#         return  [fnd_out, recon_text, recon_img, mu, log_var, cnn_enc_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MVAE(\n",
       "  (text_encoder): TextEncoder(\n",
       "    (embedding): Embedding(31299, 32)\n",
       "    (text_encoder): LSTM(32, 32, batch_first=True, bidirectional=True)\n",
       "    (text_enc_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (visual_encoder): VisualEncoder(\n",
       "    (vis_encoder): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (17): ReLU(inplace=True)\n",
       "        (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (24): ReLU(inplace=True)\n",
       "        (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (26): ReLU(inplace=True)\n",
       "        (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (31): ReLU(inplace=True)\n",
       "        (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (33): ReLU(inplace=True)\n",
       "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (35): ReLU(inplace=True)\n",
       "        (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (vis_enc_fc1): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (vis_enc_fc2): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  )\n",
       "  (text_decoder): TextDecoder(\n",
       "    (text_decoder): LSTM(32, 32, batch_first=True, bidirectional=True)\n",
       "    (latent2hidden): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (outputs2vocab): Linear(in_features=64, out_features=31298, bias=True)\n",
       "  )\n",
       "  (visual_decoder): VisualDecoder(\n",
       "    (vis_dec_fc1): Linear(in_features=32, out_features=1024, bias=True)\n",
       "    (vis_dec_fc2): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  )\n",
       "  (combined_fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc_var): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fnd_module): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import *\n",
    "final_model = MVAE(params_dict_model)\n",
    "final_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the tensorboard summary writer\n",
    "writer = SummaryWriter('runs/mvae_exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = AdamW(final_model.parameters(),\n",
    "                  lr=parameter_dict_opt['l_r'],\n",
    "                  eps=parameter_dict_opt['eps'])\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0, # Default value\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(ip_text, ip_img, ip_label, mu, log_var, rec_text, rec_img, fnd_label, lambda_wts) -> dict:\n",
    "    \"\"\"\n",
    "    Computes the VAE loss function.\n",
    "    KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"correct \", ip_text.shape)\n",
    "    print(\" rec  \", rec_text)\n",
    "    fnd_loss = fnd_loss_fn(fnd_label, ip_label)\n",
    "\n",
    "    recons_loss =F.mse_loss(ip_img, rec_img)\n",
    "\n",
    "    text_loss = nn.NLLLoss(rec_text, ip_text)\n",
    "\n",
    "    kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "    loss = lambda_wts['fnd'] * fnd_loss + lambda_wts['img'] * recons_loss + lambda_wts['kld'] * kld_loss + lambda_wts['text'] * text_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "outputs shape after lstm  torch.Size([8, 6169, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.76 GiB (GPU 0; 10.92 GiB total capacity; 7.01 GiB already allocated; 3.14 GiB free; 7.04 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9ae672496a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_dict_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameter_dict_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_models/mvae_model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FakeNewsDetection/MVAE_imp/train_val.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, scheduler, train_dataloader, val_dataloader, epochs, evaluation, device, param_dict_model, param_dict_opt, save_best, file_path, writer)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Perform a forward pass. This will return logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mfnd_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_enc_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgs_ip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Compute loss and accumulate the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FakeNewsDetection/MVAE_imp/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_ip, img_ip)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m#         print(\"text ip shape\",text_ip.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mrecon_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_ip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mfnd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FakeNewsDetection/MVAE_imp/models.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z, max_len)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mrecon_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mrecon_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FakeNewsDetection/MVAE_imp/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, max_length)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# project outputs to vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m#         logp = nn.functional.log_softmax(self.outputs2vocab(outputs.view(-1, outputs.size(2))), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs2vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logp shape before \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fn/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.76 GiB (GPU 0; 10.92 GiB total capacity; 7.01 GiB already allocated; 3.14 GiB free; 7.04 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train(model=final_model, loss_fn=loss_function, optimizer=optimizer, scheduler=scheduler, train_dataloader=train_dataloader, val_dataloader=test_dataloader, epochs=EPOCHS, evaluation=True, device=device, param_dict_model=params_dict_model, param_dict_opt=parameter_dict_opt, save_best=True, file_path='./saved_models/mvae_model.pt', writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.tensor([1, 0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(m(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 19 11:23:53 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   34C    P2    56W / 250W |   7965MiB / 11177MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      2006      G   /usr/lib/xorg/Xorg                            45MiB |\r\n",
      "|    0      9239      C   /home/muditd/.conda/envs/fn/bin/python      7907MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
